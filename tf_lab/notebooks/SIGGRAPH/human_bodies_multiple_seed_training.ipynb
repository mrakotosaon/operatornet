{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picking GPU 3\n"
     ]
    }
   ],
   "source": [
    "from general_tools.notebook.gpu_utils import setup_one_gpu\n",
    "GPU = 3\n",
    "setup_one_gpu(GPU)\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os.path as osp\n",
    "from collections import defaultdict\n",
    "\n",
    "from general_tools.notebook.tf import reset_tf_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import helper\n",
    "\n",
    "from tf_lab.diff_maps.in_out import raw_data, produce_net_data,\\\n",
    "prep_splits_labels_for_task, produce_diff_maps, classes_of_tasks\n",
    "\n",
    "from tf_lab.diff_maps.basic_nets import pc_net, diff_mlp_net, pc_versions, diff_conv_net\n",
    "from tf_lab.diff_maps.basic_nets import Basic_Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def average_l2_distance(gt_vecs, pred_vecs):\n",
    "    return np.mean(np.sqrt(np.sum(np.square((gt_vecs - pred_vecs)), axis=1)))\n",
    "\n",
    "def score_net(task, stats):\n",
    "    train_s = np.array(stats['train'])\n",
    "    test_s = np.array(stats['test'])\n",
    "    val_s = np.array(stats['val'])\n",
    "\n",
    "    if task == 'regression':\n",
    "        seek = np.min(val_s)\n",
    "    else:\n",
    "        seek = np.max(val_s)\n",
    "\n",
    "    val_best_all = np.where(val_s == seek)[0]\n",
    "    val_maximizer = val_best_all[0]\n",
    "    test_best = test_s[val_maximizer]\n",
    "    train_best = train_s[val_maximizer]\n",
    "    gen_error = test_best - train_best   \n",
    "    return len(val_best_all), val_maximizer, gen_error[0], test_best[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_mesh_dir = '/orions4-zfs/projects/optas/DATA/Meshes/SCAPE_8_poses/'\n",
    "gt_param_f = osp.join(top_mesh_dir, 'gt_shape_params.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_shapes = helper.total_shapes\n",
    "n_pose_classes = helper.n_pose_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_pc = True\n",
    "knn = 20\n",
    "arch = 'conv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_data_dir = '/orions4-zfs/projects/optas/DATA/OUT/latent_diff_maps/experiments/SCAPE_8_poses'\n",
    "synced_bases_file = osp.join(top_data_dir, '50_extract_%d_knn_50_fmapd.mat' % (knn,) )\n",
    "\n",
    "if arch == 'mlp':\n",
    "    n_consistent = [5, 10, 20, 30, 40, 50]\n",
    "else:\n",
    "    n_consistent = [20, 40, 50]\n",
    "    \n",
    "seeds = [42, 100, 666, 1821, 2004]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_member_per_class = 50\n",
    "n_shapes = sub_member_per_class * n_pose_classes\n",
    "val_per = 0.10\n",
    "test_per = 0.15\n",
    "train_per = 1.0 - (val_per + test_per)\n",
    "n_pc_points = 1024\n",
    "task = 'regression'\n",
    "mean_norm_diffs = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if use_pc:\n",
    "    learning_rate = 0.005\n",
    "    batch_size = 50\n",
    "    pc_version = 'v2'\n",
    "else:\n",
    "    learning_rate = 0.007\n",
    "    batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gt_latent_params, in_pcs, pose_labels = \\\n",
    "raw_data(top_mesh_dir, gt_param_f, sub_member_per_class, n_pc_points)\n",
    "\n",
    "n_classes = classes_of_tasks(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# n_consistent = [0.001, 0.002, 0.005, 0.007, 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "100\n",
      "666\n",
      "1821\n",
      "2004\n"
     ]
    }
   ],
   "source": [
    "verbose = False\n",
    "train_epochs = 1000\n",
    "stats_over_reps = defaultdict(list)\n",
    "\n",
    "for seed in seeds:\n",
    "    print seed\n",
    "    np.random.seed(seed)\n",
    "    for n_cons in n_consistent:\n",
    "        # Prepare splits\n",
    "        diff_maps = produce_diff_maps(synced_bases_file, n_cons, n_shapes)\n",
    "        splits, labels = \\\n",
    "        prep_splits_labels_for_task(task, gt_latent_params, pose_labels, train_per, test_per, seed)\n",
    "        net_data = produce_net_data(in_pcs, splits, labels, diff_maps, use_pc, mean_norm_diffs)\n",
    "        \n",
    "        # Prepare graph\n",
    "        reset_tf_graph()\n",
    "        tf.set_random_seed(seed)\n",
    "        if use_pc:\n",
    "            n_filters, n_neurons = pc_versions(pc_version)\n",
    "            net_out, feed_pl, label_pl = pc_net(n_pc_points, task, n_filters, n_neurons, verbose)\n",
    "        else:\n",
    "            if arch == 'mlp':\n",
    "                net_out, feed_pl, label_pl = diff_mlp_net(n_cons, task, verbose)\n",
    "            elif arch == 'conv':\n",
    "                net_out, feed_pl, label_pl = diff_conv_net(n_cons, task, verbose)\n",
    "        \n",
    "        net = Basic_Net(net_out, feed_pl, label_pl)\n",
    "        net.define_loss(task)\n",
    "        net.define_opt(learning_rate)\n",
    "        net.start_session()        \n",
    "        \n",
    "        # Train\n",
    "        stats = net.train(train_epochs, batch_size, net_data, task, verbose=verbose)\n",
    "        n_opt, val_maximizer, gen_error, test_best = score_net(task, stats)    \n",
    "        if verbose:\n",
    "            print n_opt, val_maximizer, gen_error, test_best    \n",
    "        stats_over_reps[n_cons].append([n_opt, val_maximizer, gen_error, test_best])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 [  3.204e+02   3.106e-02   6.364e-02] [ 0.014  0.01 ]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "for n_cons in n_consistent:\n",
    "    print n_cons, np.mean(stats_over_reps[n_cons], axis=0)[1:], np.std(stats_over_reps[n_cons], axis=0)[2:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow1",
   "language": "python",
   "name": "tf1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
